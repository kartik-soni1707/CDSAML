{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA EXPLORATION ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_df = pd.read_csv(\"lgtmt_tweets.csv\")# reading spam tweets and dropping those with Nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam = spam_df['Tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2353468"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    MELBOURNE ENQUIRY: Seeking a variety of acts f...\n",
       "1    THE BURLESQUE BOOTCAMP SYDNEY - Open Date tick...\n",
       "2    THE BURLESQUE BOOTCAMP SYDNEY - Open Date tick...\n",
       "3    THE BURLESQUE BOOTCAMP SYDNEY - Open Date tick...\n",
       "4    Come to \"The Burlesque Bootcamp - Sydney\" Satu...\n",
       "5    21st Century Pinups write about our girls perf...\n",
       "6    The Burlesque Bootcamp â€“ Coming to Sydney!:   ...\n",
       "7    ATTN MELBOURNE: We have some group specials av...\n",
       "8    Story on our lovely Vivi Valentine, on contrac...\n",
       "9    and also Black Flamingo in Berlin with Catheri...\n",
       "Name: Tweet, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will now try to look at the amount of URLs and mentions in the Spam Dataset ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1620227\n"
     ]
    }
   ],
   "source": [
    "urls = 0\n",
    "for tweet in spam:\n",
    "    try:\n",
    "        if re.search(\"http\", tweet) != None:\n",
    "            urls = urls + 1\n",
    "    except:\n",
    "        print(tweet)\n",
    "        \n",
    "print(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Around 68.84423327616946% of spam tweets have URLs in them\n"
     ]
    }
   ],
   "source": [
    "print(\"Around {}% of spam tweets have URLs in them\".format(urls/len(spam)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "411401\n",
      "Around 17.48062858725931% of spam tweets have mentions in them\n"
     ]
    }
   ],
   "source": [
    "mentions = 0\n",
    "for tweet in spam:\n",
    "    try:\n",
    "        if re.search(\"@\", tweet) != None:\n",
    "            mentions += 1\n",
    "    except:\n",
    "        print(tweet)\n",
    "        \n",
    "print(mentions)\n",
    "print(\"Around {}% of spam tweets have mentions in them\".format(mentions/len(spam)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Lexical Richness* ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First we will try to tokenize the tweets and perform basic cleaning on them ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "import nltk\n",
    "lm = WordNetLemmatizer()\n",
    "import re\n",
    "\n",
    "# converts symbols from nltk format to wordnet format.\n",
    "# Taken from stackoverflow https://stackoverflow.com/questions/15586721/wordnet-lemmatization-and-pos-tagging-in-python\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return 'n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MELBOURNE ENQUIRY: Seeking a variety of acts for our end of year show. Payment is $120 per slot or $200 for 2.... '"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r'(http://)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)', '',spam[0])\n",
    "# Removed the URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r'(http://)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)', '',\"www.google.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MELBOURNE ENQUIRY Seeking a variety of acts for our end of year show Payment is  per slot or  for  httpbitlyAhfF'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r'[\\.\\\\/\\(\\),\\-!@#$%^&*~`:0-9\\?]', '', spam[0])\n",
    "# Removes all the special characters I could think of (including numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a list \n",
    "def clean(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    tweet = re.sub(r'(http://)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)', '',tweet)\n",
    "    tweet = re.sub(r'[\\.\\\\/\\(\\),\\-!@#$%^&*~`:0-9\\?+=-\\[\\]\\{\\};\\'\\\"<>]', '', tweet)\n",
    "    tweet = tweet.split()\n",
    "    tweet = nltk.pos_tag(tweet) # adds the \"part of sentence (noun, verb, adjective, etc )\" after each word\n",
    "    tweet = [lm.lemmatize(x[0], get_wordnet_pos(x[1])) for x in tweet]\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_vocab = set()\n",
    "for tweet in spam[:1000]:\n",
    "    tweet = clean(tweet)\n",
    "    spam_vocab.update(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2499"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(spam_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "ttr = []\n",
    "for tweet in spam[:1000]:\n",
    "    tweet = clean(tweet)\n",
    "    try:\n",
    "        ttr.append(len(set(tweet))/len(tweet))\n",
    "    except:\n",
    "        print(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8947368421052632,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.8571428571428571,\n",
       " 1.0,\n",
       " 0.6,\n",
       " 1.0,\n",
       " 0.9333333333333333,\n",
       " 0.9230769230769231]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttr[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.954053551603292"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average(ttr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's try the same on not-spam tweets ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham = pd.read_csv('spam_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = ham['Tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "673166\n"
     ]
    }
   ],
   "source": [
    "urls = 0\n",
    "for tweet in tweets:\n",
    "    try:\n",
    "        if re.search(\"http\", tweet) != None:\n",
    "            urls = urls + 1\n",
    "    except:\n",
    "        print(tweet)\n",
    "        \n",
    "print(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Around 20.651221235387034% of spam tweets have URLs in them\n"
     ]
    }
   ],
   "source": [
    "print(\"Around {}% of ham tweets have URLs in them\".format(urls/len(tweets)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a big drop compared to almost 70 % in spam tweets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1489738\n",
      "Around 45.70181652187278% of ham tweets have mentions in them\n"
     ]
    }
   ],
   "source": [
    "mentions = 0\n",
    "for tweet in tweets:\n",
    "    try:\n",
    "        if re.search(\"@\", tweet) != None:\n",
    "            mentions += 1\n",
    "    except:\n",
    "        print(tweet)\n",
    "        \n",
    "print(mentions)\n",
    "print(\"Around {}% of ham tweets have mentions in them\".format(mentions/len(tweets)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It appears that spam tweets actually have less mentions than those not spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "ttr = []\n",
    "for tweet in tweets[:1000]:\n",
    "    tweet = clean(tweet)\n",
    "    try:\n",
    "        ttr.append(len(set(tweet))/len(tweet))\n",
    "    except:\n",
    "        print(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9664079833392205"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average(ttr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The TTR ratio only seems to be slightly higher in ham than spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3725\n"
     ]
    }
   ],
   "source": [
    "ham_vocab = set()\n",
    "for tweet in tweets[:1000]:\n",
    "    tweet = clean(tweet)\n",
    "    ham_vocab.update(tweet)\n",
    "    \n",
    "print(len(ham_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The vocabulary itself seems to be much richer ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ham vocab is 49.05962384953982% larger than spam vocab\n"
     ]
    }
   ],
   "source": [
    "print(\"Ham vocab is {}% larger than spam vocab\".format((3725-2499)/2499*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next we can try to see the diversity and similarity between the words used in the two vocabs using a pretrained word2vec model ### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
